
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>How to build an image similarity system</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="image-similarity-system"
                  title="How to build an image similarity system"
                  environment="web"
                  feedback-link="https://github.com/recohut/reco-step/issues">
    
      <google-codelab-step label="Introduction" duration="5">
        <p>Recommending visually similar products is an important task in today&#39;s E-commerce recommender systems. In this tutorial, we will learn how to achieve this task using the latest tools and techniques.</p>
<h2 is-upgraded>What you&#39;ll learn?</h2>
<ol type="1">
<li>Computer vision image classification model fine-tuning</li>
<li>Vector indexing and retrieval</li>
<li>Flask API creation</li>
<li>Deep learning model deployment on AWS Beanstalk</li>
</ol>
<h2 is-upgraded>Why is this important?</h2>
<ul>
<li>An end-to-end process of ML</li>
<li>Visually similar product recommendations are based on this system</li>
</ul>
<h2 is-upgraded>How it will work?</h2>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-how-to-build-an-image-similarity-system-untitled.png" src="img/7b3d68bd58c690aa.png"></p>
<h2 is-upgraded>Who is this for?</h2>
<ul>
<li>People who are new in deep learning and computer vision</li>
<li>People looking to fine-tune (transfer learning) and deploy image similarity systems</li>
</ul>
<h2 is-upgraded>Important resources</h2>
<ul>
<li><a href="https://nb-dev.recohut.com/similarity/visual/2021/04/27/image-similarity-recommendations.html" target="_blank">Notebook</a></li>
<li><a href="https://nb-dev.recohut.com/similarity/visual/retail/2021/04/23/similar-product-recommender.html" target="_blank">Notebook</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="D
ataset" duration="5">
        <p>We listed down 3 datasets from Kaggle that was best fitting the criteria of this use case: 1) <a href="https://www.kaggle.com/bhaskar2443053/fashion-small?" target="_blank">Fashion Product Images (Small)</a>, 2) <a href="https://www.kaggle.com/trolukovich/food11-image-dataset?" target="_blank">Food-11 image dataset</a> and 3) <a href="https://www.kaggle.com/jessicali9530/caltech256?" target="_blank">Caltech 256 Image Dataset</a>. I selected the Fashion dataset and Foods dataset.</p>
<p>Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Encoder fine-tuning" duration="10">
        <p>Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use.</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-how-to-build-an-image-similarity-system-untitled-1.png" src="img/563fd11f6bd3430d.png"></p>
<p>Fig: a screenshot of encoder fine-tuning process</p>


      </google-codelab-step>
    
      <google-codelab-step label="Image vectorization" duration="10">
        <p>Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use.</p>
<p>We can select any pre-trained image classification model. These models are commonly known as encoders because their job is to encode an image into a feature vector. I analyzed four encoders named 1) MobileNet, 2) EfficientNet, 3) ResNet and 4) <a href="https://tfhub.dev/google/bit/m-r152x4/1" target="_blank">BiT</a>. After basic research, I decided to select BiT model because of its performance and state-of-the-art nature. I selected the BiT-M-50x3 variant of model which is of size 748 MB. More details about this architecture can be found on the official page <a href="https://tfhub.dev/google/bit/m-r50x3/1" target="_blank">here</a>.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Metadata &amp; indexing" duration="10">
        <p>Images are represented in a fixed-length feature vector format. For the given input vector, we need to find the TopK most similar vectors, keeping the memory efficiency and real-time retrival objective in mind. I explored the most popular techniques and listed down five of them: Annoy, Cosine distance, L1 distance, Locally Sensitive Hashing (LSH) and Image Deep Ranking. I selected Annoy because of its fast and efficient nature. More details about Annoy can be found on the official page <a href="https://github.com/spotify/annoy" target="_blank">here</a>.</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-how-to-build-an-image-similarity-system-untitled-2.png" src="img/cf50d92063b73ca2.png"></p>
<p>We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index objects for later use.</p>


      </google-codelab-step>
    
      <google-codelab-step label="API" duration="10">
        <p>We will receive an image from user, encode it with our image encoder, find TopK similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Deployment" duration="10">
        <p>The API was deployed on AWS cloud infrastructure using AWS Elastic Beanstalk service.</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-how-to-build-an-image-similarity-system-untitled-3.png" src="img/cf687d0e5458e19b.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion" duration="2">
        <p>Congratulations!</p>
<h2 is-upgraded>Links and References</h2>
<ol type="1">
<li>Determining Image similarity with Quasi-Euclidean Metric <a href="https://arxiv.org/abs/2006.14644v1" target="_blank">arxiv</a></li>
<li>CatSIM: A Categorical Image Similarity Metric <a href="https://arxiv.org/abs/2004.09073v1" target="_blank">arxiv</a></li>
<li>Central Similarity Quantization for Efficient Image and Video Retrieval <a href="https://arxiv.org/abs/1908.00347v5" target="_blank">arxiv</a></li>
<li>Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval <a href="https://arxiv.org/abs/1803.02987v3" target="_blank">arxiv</a></li>
<li>Model-based Behavioral Cloning with Future Image Similarity Learning <a href="https://arxiv.org/abs/1910.03157v1" target="_blank">arxiv</a></li>
<li>Why do These Match? Explaining the Behavior of Image Similarity Models <a href="https://arxiv.org/abs/1905.10797v1" target="_blank">arxiv</a></li>
<li>Learning Non-Metric Visual Similarity for Image Retrieval <a href="https://arxiv.org/abs/1709.01353v2" target="_blank">arxiv</a></li>
</ol>
<h2 is-upgraded>Have a Question?</h2>
<ul>
<li><a href="https://form.jotform.com/211377288388469" target="_blank">Fill out this form</a></li>
<li><a href="https://github.com/recohut/reco-step/issues" target="_blank">Raise issue on Github</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
