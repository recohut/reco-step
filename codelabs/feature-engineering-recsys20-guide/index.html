
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>How to do feature engineering for recommenders</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="feature-engineering-recsys20-guide"
                  title="How to do feature engineering for recommenders"
                  environment="web"
                  feedback-link="https://github.com/recohut/reco-step/issues">
    
      <google-codelab-step label="Introduction" duration="5">
        <p><strong><em>RecSys2020 Tutorial: Feature Engineering for Recommender Systems</em></strong></p>
<p>The selection of features and proper preparation of data for deep learning or machine learning models plays a significant role in the performance of recommender systems. To address this we propose a tutorial highlighting best practices and optimization techniques for feature engineering and preprocessing of recommender system datasets. The tutorial will explore feature engineering using pandas and Dask, and will also cover acceleration on the GPU using open source libraries like RAPIDS and NVTabular. Proposed length is 180min. We&#39;ve designed the tutorial as a combination of a lecture covering the mathematical and theoretical background and an interactive session based on jupyter notebooks. Participants will practice the discussed features by writing their own implementation in Python. NVIDIA will host the tutorial on their infrastructure, providing dataset, jupyter notebooks and GPUs. Participants will be able to easily attend the tutorial via their web browsers, avoiding any complicated setup.</p>
<p>Beginner to intermediate users are the target audience, which should have prior knowledge in python programming using libraries, such as pandas and NumPy. In addition, they should have a basic understanding of recommender systems, decision trees and feed forward neural networks.</p>
<h2 is-upgraded>What you&#39;ll learn?</h2>
<ol type="1">
<li>Different methods to create new features from raw data</li>
<li>Handle big data</li>
<li>Using GPU for faster data processing</li>
</ol>
<h2 is-upgraded>Why is this important?</h2>
<p>Deep learning and advanced modeling techniques extract features from images and text but still faces difficulty in extracting quality features from tabular data. For this, we still need to learn and apply feature engineering techniques.</p>
<h2 is-upgraded>How it will work?</h2>
<ol type="1">
<li>ETL the raw data</li>
<li>Exploratory analysis</li>
<li>Feature engineering techniques</li>
<li>Time series feature techniques</li>
<li>Install and use RapidsAI&#39;s cuDF for faster processing</li>
</ol>
<h2 is-upgraded>Who is this for?</h2>
<ul>
<li>People who are new in deep learning and feature engineering</li>
<li>People looking to improve their recommender models by adding more quality features</li>
</ul>
<h2 is-upgraded>Important resources</h2>
<ul>
<li><a href="https://nb-dev.recohut.com/features/recsys/cudf/retail/bigdata/2021/06/19/recsys20-tutorial-feature-engineering-part-1.html" target="_blank">Notebook - Recsys&#39;20 Feature Engineering Tutorial Part 1</a></li>
<li><a href="https://nb-dev.recohut.com/features/recsys/cudf/retail/bigdata/2021/06/19/recsys20-tutorial-feature-engineering-part-2.html" target="_blank">Notebook - Recsys&#39;20 Feature Engineering Tutorial Part 2</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Define the goal" duration="5">
        <p>Events: View, AddToCart, Purchase</p>
<p>Timeframe: Oct-2019 - April 2020</p>
<p>As the dataset contains only interactions (positive samples), we need to define a goal/target to predict. There is a lot of literature about how to construct negative samples from the dataset in order to make the goal easier or harder to predict.</p>
<p>We define our own goal and filter the dataset accordingly. For our tutorial, we decided that our goal is to predict if a user purchased an item:</p>
<ul>
<li>Positive: User purchased an item</li>
<li>Negative: User added an item to the cart, but did not purchase it (in the same session)</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Dataset" duration="10">
        <p>In our tutorial, we use the <a href="https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store" target="_blank">eCommerce behavior data</a> from the multi-category store from the <a href="https://rees46.com/" target="_blank">REES46 Marketing Platform</a> as our dataset, containing the user behavior (view/add to cart/ purchase) of an e-commerce shop over 7 months.</p>
<h2 is-upgraded>Download the data</h2>
<p>Oct&#39;19 and Nov&#39;19 data is on <a href="https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store" target="_blank">Kaggle</a>.</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled.png" src="img/af5f2ca1eb8d7bc4.png"></p>
<p>Dec&#39;19 to Apr&#39;20 data is available in <a href="https://drive.google.com/drive/folders/1Nan8X33H8xrXS5XhCKZmSpClFTCJsSpE" target="_blank">this</a> drive.</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-1.png" src="img/6e272a0a23ef00b0.png"></p>
<p><strong><em>Importance advice</em></strong></p>
<p><em>: One piece of advice is to use cuDF instead of pandas. Analyzing the dataset requires calculating different groupby combination multiple times by a data scientist. GPU acceleration can significantly speed up the calculations and enables you to run more comparisons.</em></p>
<h2 is-upgraded>Statistics</h2>
<p>This dataset contains 285 million users&#39; events from the eCommerce website.</p>
<h2 is-upgraded>Data split</h2>
<p>We split the dataset into train, validation and test set by the timestamp:</p>
<ul>
<li>Training: October 2019 - February 2020</li>
<li>Validation: March 2020</li>
<li>Test: April 2020</li>
</ul>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-2.png" src="img/251a73a030179f38.png"></p>
<h2 is-upgraded>Pre-filtering</h2>
<p>We remove AddToCart Events from a session, if in the same session the same item was purchased.</p>
<h2 is-upgraded>Transformation</h2>
<pre><code language="language-python" class="language-python">def process_files(df_tmp, chunkname):
    df_tmp[&#39;session_purchase&#39;] =  df_tmp[&#39;user_session&#39;] + &#39;_&#39; + df_tmp[&#39;product_id&#39;].astype(str)
    df_purchase = df_tmp[df_tmp[&#39;event_type&#39;]==&#39;purchase&#39;]
    df_cart = df_tmp[df_tmp[&#39;event_type&#39;]==&#39;cart&#39;]
    df_purchase = df_purchase[df_purchase[&#39;session_purchase&#39;].isin(df_cart[&#39;session_purchase&#39;])]
    df_cart = df_cart[~(df_cart[&#39;session_purchase&#39;].isin(df_purchase[&#39;session_purchase&#39;]))]
    df_cart[&#39;target&#39;] = 0
    df_purchase[&#39;target&#39;] = 1
    df = pd.concat([df_cart, df_purchase])
    df = df.drop(&#39;category_id&#39;, axis=1)
    df = df.drop(&#39;session_purchase&#39;, axis=1)
    # df[[&#39;cat_0&#39;, &#39;cat_1&#39;, &#39;cat_2&#39;, &#39;cat_3&#39;]] = df[&#39;category_code&#39;].str.split(&#34;\.&#34;, n = 3, expand = True).fillna(&#39;NA&#39;)
    df[&#39;brand&#39;] = df[&#39;brand&#39;].fillna(&#39;NA&#39;)
    # df = df.drop(&#39;category_code&#39;, axis=1)
    df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;event_time&#39;].str.replace(&#39; UTC&#39;, &#39;&#39;))
    df[&#39;ts_hour&#39;] = df[&#39;timestamp&#39;].dt.hour
    df[&#39;ts_minute&#39;] = df[&#39;timestamp&#39;].dt.minute
    df[&#39;ts_weekday&#39;] = df[&#39;timestamp&#39;].dt.weekday
    df[&#39;ts_day&#39;] = df[&#39;timestamp&#39;].dt.day
    df[&#39;ts_month&#39;] = df[&#39;timestamp&#39;].dt.month
    df[&#39;ts_year&#39;] = df[&#39;timestamp&#39;].dt.year
    df.to_csv(chunkname, index=False)
</code></pre>
<h2 is-upgraded>ETL</h2>
<p>We downloaded the datasets, extracted them, applied transformation, applied split, and exported in parquet files. Here is how we will load this dataset for EDA and modeling:</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-3.png" src="img/754e24ae6f04fc70.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="EDA" duration="10">
        <h2 is-upgraded>Sparse feature frequence analysis</h2>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-4.png" src="img/b017011860cf530d.png"></p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-5.png" src="img/81cb07e36164d991.png"></p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-6.png" src="img/3f27b9d4cbc0b130.png"></p>
<p>We can observe following pattern:</p>
<ul>
<li>There are ~45000 products which appear only once in the dataset</li>
<li>There are 1200000 users which appear only once in the dataset</li>
<li>There are 350 brands which appears only once in the dataset</li>
</ul>
<p>The sparsity is important for understanding which features can be better used in a model. Product_id and User_id have many values which appear only once and the model is less able to learn a good patterns from them. On the other hand, brands has many observations and can be leveraged for prediction.</p>
<h2 is-upgraded>Top frequency items analysis</h2>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-7.png" src="img/c1e5d2e4d42e191.png"></p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-8.png" src="img/9f806b2ba4ce8345.png"></p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-9.png" src="img/609ff1a509caf0fa.png"></p>
<ul>
<li>We explored the data and saw the different raw features available in the dataset.</li>
<li>We analzyed basic statistics of the raw features and saw long-tail distribution for categorical features (user, item, brand)</li>
<li>Some categorical features (categories) have high occurances</li>
<li>In general, we see that categorical features have variance in the target, which we can leverage to engineer more powerful features</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Missing Value Handling" duration="5">
        <p>Categorical Features: Imputing categorical features is easy - a unique category value (e.g. &#34;UNKNOWN&#34;) can be imputed</p>
<p>Important: Before imputing the missing values, it is beneficial to create a indicator column, which indicate if the a value was imputed or not. There is maybe a underlying pattern for the missing values and models can learn the pattern.</p>
<pre><code language="language-python" class="language-python">cols = [&#39;brand&#39;, &#39;user_session&#39;, &#39;cat_0&#39;, &#39;cat_1&#39;, &#39;cat_2&#39;, &#39;cat_3&#39;]

for col in cols:
    df_train[&#39;NA_&#39; + col] = df_train[col].isna().astype(np.int8)
    df_train[col].fillna(&#39;UNKNOWN&#39;, inplace=True)
</code></pre>
<p>Numerical Features: Imputing median for the numerical value (per group) Imputing mean for numercial value (per group) In some cases, we may know what value should be used as the default value (e.g. 0 for historical data or the max)</p>
<p>Important: For the same reason as in the categorical case, it is important to add a indicator column that the datapoint was imputed.</p>
<p>In our case, we do not have missing values in the numerical column price. Therefore, we artificially inject nans and then compare the difference.</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-10.png" src="img/4cd71047d8e50460.png"></p>
<p>Predicting missing values: In <a href="https://arxiv.org/abs/2002.05515" target="_blank">Improving Deep Learning For Airbnb Search</a>, the authors propose to use a DNN for missing user engagement features of new items (listenings). New items have no historical user engagements, such as # of views, # of bookings, etc.. In the paper, they train a DNN based on the meta information, such as price, location and predict the user engagements feature. This could be interpreted in what are the expected user engagement.</p>
<p>Instead of the hand-crafted default values for missing user engagement, the authors replaced the missing values with the prediction of the DNN and showed that it reduced the error by 43% (offline test) and improved the overall bookings by 0.38% (online A/B test).</p>


      </google-codelab-step>
    
      <google-codelab-step label="Feature Cross" duration="10">
        <p><em>Combining Categories (CC)</em> is a simple, powerful technique, but often undervalued. We will use this strategy in other feature engineering techniques, as well, and will introduce its value in a simple example.</p>
<p>In some datasets, categories by itself provide no information to predict the target. But if we combine multiple categories, together, then we can indentify patterns.</p>
<p>For example, we have the following categories:</p>
<ul>
<li>Weekday</li>
<li>Hour of the day</li>
</ul>
<p>Each of them independently has no significant pattern in the dataset. If we combine them with Weekday_HourOfTheDay, then we can observe some strong behavior for certainn times on the weekend</p>
<p>Decision Trees determine the split in the dataset on single features. If each categorical feature by itself does not provide the information gain, then Decision Trees cannot find a good split. If we provide a combined categorical feature, the Decision Tree can easier split the dataset.</p>
<p>Combining categories also called Cross Column or Cross Product, is used in the Wide Deep Architecture by Google and is implemented in Tensorflow.</p>
<h2 is-upgraded>Best Practices!</h2>
<ul>
<li>Combining low cardinal categories is a good start. For example, the dataset size is 100M rows and there are multiple categories with a caridnality (# of unique values) of 10-50, then combining them should not result in low observation count</li>
<li>Exploratory Data Analysis (EDA) is faster than training a model. Analyzing the information value for different combination of categorical features (on a sample) is really fast.</li>
</ul>
<h2 is-upgraded>Implementation</h2>
<pre><code language="language-python" class="language-python">def explore_cat(df, cats):
    df_agg = df_train[cats + [&#39;target&#39;]].groupby(cats).agg([&#39;mean&#39;, &#39;count&#39;]).reset_index()
    df_agg.columns = cats + [&#39;mean&#39;, &#39;count&#39;]
    print(df_agg.sort_values(&#39;count&#39;, ascending=False).head(20))
    
cats = [&#39;product_id&#39;, &#39;user_id&#39;]  
explore_cat(df_train, cats)
</code></pre>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-11.png" src="img/1166831f07b38392.png"></p>
<h2 is-upgraded>Hypothesis based on the outcome</h2>
<ul>
<li>Some user will always buy the same one-way products e.g. cleaning supplies, food</li>
<li>Behavior changes on weekday+hour - e.g. during the week, users will not stay up late as they work next day</li>
<li>Category and brand are both powerful features, but the combination can be more important. E.g. do people buy apple smartphones or accessories?</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Categorify" duration="5">
        <p><em>Categorifying</em> is required for using categorical features in deep learning models with Embedding layers. An Embedding layer encodes the category into a hidden latent vector with a smaller dimension.</p>
<p>Categorical features can be from datatype String or Integer. The Embedding layer requires that categorical features are continuous, positive Integers from 0 to |C| (number of unique category values).</p>
<p>Another important reason to Categorify categorical features is to reduce the size of the dataset. Often categorical features are of the data type String and sometimes, they are hashed to protect the user/dataset privacy. For example, we can hash the Integer 0 to an md5 hash.</p>
<p>Finally, we can prevent overfitting for low frequency categories. Categories with low frequency can be grouped together to an new category called ‘other&#39;. In the previous exercise we learned that it is powerful to combine categorical features together to create a new feature. However, combining categories increases the cardinality of the new feature and the number of observations per category will decrease. Therefore, we can apply a treshhold to group all categories with lower frequency count to the the new category.</p>
<p>In addition, categories, which occurs in the validation dataset and do not occur in the training dataset, should be mapped to the ‘other&#39; category as well.</p>
<p>We use in our example the category Ids 0 or 1 for a placeholder for the low frequency and unkown category. Then our function is independent of the cardinality of the categorical feature and we do not keep records of the cardinality to know the low frequency/unkown category.</p>
<p>In our dataset, we see that multiple product_ids occur only once in the training dataset. Our model would overfit to these low frequency categories.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Target Encoding" duration="5">
        <p><em>Target Encoding (TE)</em> calculates the statistics from a target variable grouped by the unique values of one or more categorical features.</p>
<p>For example in a binary classification problem, it calculates the probability that the target is true for each category value - a simple mean.</p>
<p>Target Encoding creates a new features, which can be used by the model for training. The advantage of Target Encoding is, that it process the categorical features and makes them easier accessible to the model during training and validation.</p>
<p>Tree-based model requires to create a split for each categorical value (depending on the exact model). Target Encoding saves to create many splits for the model. In particular, when applying Target Encoding to multiple columns, it reduces significantly the number of splits. The model can directly operate on the probablities/averages and creates a split based on them. Another advantage is, that some boosted-tree libraries, such as XGBoost, cannot handle categorical features. The library requires to hot-n encode them. Categorical features with large cardinality (e.g. &gt;100) are inefficient to store as hot-n.</p>
<p>Deep learning models often apply Embedding Layers to categorical features. Embedding layer can overfit quickly and categorical values with low frequencies have ony a few gradient descent updates and can memorize the training data.</p>
<h2 is-upgraded>Smoothing</h2>
<p>The introduced Target Encoding is a good first step, but it lacks to generalize well and it will tend to overfit, as well. We can observe, that the observation count for some categories are 1. This means, that we have only one data point to calculate the average and Target Encoding overfits to these values. Therefore, we need to adjust the calculation:</p>
<ul>
<li>if the number of observation is high, we want to use the mean of this category value</li>
<li>if the number of observation is low, we want to use the global mean</li>
</ul>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-12.png" src="img/e6798bda072010b9.png"></p>
<h2 is-upgraded><strong>Improve TargetEncoding with out-of-fold</strong></h2>
<p>We can still improve our Target Encoding function. We can even make it more generalizable, if we apply an out of fold calculation.</p>
<p>In our current definition, we use the full training dataset to Target Encode the training dataset and validation/test dataset. Therefore, we will likely overfit slightly on our training dataset, because we use the information from it to encode the categorical values. A better strategy is to use out of fold:</p>
<ul>
<li>use the full training dataset to encode the validation/test dataset</li>
<li>split the training dataset in k-folds and encode the i-th fold by using all folds except of the i-th one</li>
</ul>
<p>The following figure visualize the strategy for k=5:</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-13.png" src="img/24c38329693d1a2c.png"></p>
<h2 is-upgraded>Summary</h2>
<ul>
<li>Target Encoding calculates statistics of a target column given one or more categorical features</li>
<li>Target Encoding smooths the statistics as a weighted average of the category value and the global statistic</li>
<li>Target Encoding uses a out-of-fold strategy to prevent overfitting to the training dataset.</li>
</ul>
<p>We can see the advantage of using Target Encoding as a feature engineering step. A tree-based model or a neural network learns the average probability for the category value. However, neither model is designed to prevent overfitting.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Count Encoding" duration="5">
        <p><em>Count Encoding (CE)</em> calculates the frequency from one or more categorical features given the training dataset.</p>
<p>For example we can consider Count Encoding as the populiarity of an item or activity of an user.</p>
<p>Count Encoding creates a new feature, which can be used by the model for training. It groups categorical values based on the frequency together.</p>
<p>For example,</p>
<p>users, which have only 1 interaction in the datasets, are encoded with 1. Instead of having 1 datapoint per user, now, the model can learn a behavior pattern of these users at once. products, which have many interactions in the datasets, are encoded with a high number. The model can learn to see them as top sellers and treat them, accordingly.</p>
<p>The advantage of Count Encoding is that the category values are grouped together based on behavior. Particularly in cases with only a few observation, a decision tree is not able to create a split and neural networks have only a few gradient descent updates for these values.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Binning" duration="5">
        <p><em>Binning</em> maps multiple ordinal categorical or numerical features into groups. It is mainly applied to numerical features:</p>
<ul>
<li>prevent overfitting by grouping values together</li>
<li>enables us to add some expert knowledge into the model</li>
<li>most simple case: binary flags, e.g. features is greater than 0</li>
</ul>
<p>Examples:</p>
<ul>
<li>binning weekdays into weekday and weekend</li>
<li>binning hours into morning, early afternoon, late afternoon, evening and night</li>
<li>binning age into child, adlult and retired</li>
</ul>
<p>We can take a look on the hour of the day. We can see multiple patterns:</p>
<ul>
<li>0-3 Night: Low purchase probability</li>
<li>4-7 Early morning: Mid purchase probability</li>
<li>8-14 Morning/Lunch: Higher purchase probability</li>
<li>15-20 Afternoon: Low purchase probability</li>
<li>21-23: Evening: High purchase probability</li>
</ul>
<p>Binning the numerical features reduces the cardinality (# of unique values). Therefore, a model can easier learn the relationship to the target variables, as there are more observation per category. In addition, binning prevents overfitting.</p>
<p>Another reason to apply binning is to standardize numeric variables per category group. The datasets provides information about the product category (cat_1) and price information.</p>
<p>For example, the headphones and smartphones have a different price distribution.</p>
<ul>
<li>We can probably buy good headphones between 100−200</li>
<li>For a good smartphone, prices are probably in the range of 400−1200</li>
</ul>
<p>Therefore, the buying behavior should be different depending on the price per category (what is a good deal).</p>
<h2 is-upgraded>Apply binning</h2>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-14.png" src="img/42a5d271d94a5e75.png"></p>
<p>It is maybe counterintuitive:</p>
<ul>
<li>the highest days are Sunday and Monday - a hypothesis could be that people shop on Sunday evening and the first day of the week</li>
<li>the lowest days are Thur-Sat - a hypothesis could be that Thu/Fri is end of week and people are finishing up their work and have no time to do online shopping</li>
<li>Saturday is maybe a day go outside</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Normalization" duration="5">
        <p><em>Normalization</em> is required to enable neural networks to leverage numerical features. Tree-based models do not require normalization as they define the split independent of the scale of a feature. Without normalization, neural networks are difficult to train. The image visualizes the loss surface and the gradient updates for non-normalized input (left) and normalized input (right).</p>
<p>We will first generate some numerical features with the feature engineering that we also covered in previous steps.</p>
<p>The reason is that different numerical features have different scales. When we combine the features in a hidden layer, the different scales make it more difficult to extract patterns from it.</p>
<p>Normalization Techniques After we outline the importance for normalizing the numerical input feature, we will discuss different strategy to achieve a normal distributed input feature:</p>
<ul>
<li>Normalization with mean/std</li>
<li>Log-based normalization</li>
<li>Scale to 0-1</li>
<li>Gauss Rank (separate notebook)</li>
<li>Power transfomer</li>
</ul>
<h2 is-upgraded>Gauss Rank</h2>
<p>Gauss Rank transforms any arbitrary distribution to a Gaussian normal distribution by</p>
<ol type="1">
<li>Compute the rank (or sort the values ascending)</li>
<li>Scale the values linearly from -1 to +1</li>
<li>Apply the erfinv function</li>
</ol>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-15.png" src="img/8034e737d9ecd6d6.png"></p>
<p>Let&#39;s normalize the features price, TE_ts_weekday_ts_hour_cat_2_brand and CE_cat_2 with GaussRank, and plot the non-normalized and normalized values</p>
<pre><code language="language-python" class="language-python">fig, axs = plt.subplots(3, 2, figsize=(16,9))
for i, col in enumerate([&#39;price&#39;, &#39;TE_ts_weekday_ts_hour_cat_2_brand&#39;, &#39;CE_cat_2&#39;]):
    data_sample = df_train[col].sample(frac=0.01)
    axs[i, 0].hist(data_sample.to_pandas(), bins=50)
    axs[i, 1].hist(cp.asnumpy(gaussrank_gpu(data_sample.values)), bins=50)
    if i==0:
        axs[i, 0].set_title(&#39;Histogram non-normalized&#39;)
        axs[i, 1].set_title(&#39;Histogram Gauss Rank&#39;)
</code></pre>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-feature-engineering-for-recommenders-untitled-16.png" src="img/1fae908af391a778.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Timeseries events" duration="5">
        <p>Many real-world recommendation systems contain time information. The system normally logs events with a timestamp. Tree-based or deep learning based models usually only uses the information from the datapoint itself for the prediction and they have difficulties to capture relationships over multiple datapoints.</p>
<p>Let&#39;s take a look at a simple example. Let&#39;s assume we have the interaction events of an itemid, userid and action with the timestamp.</p>
<p>We can extract many interesting features based on the history, such as</p>
<ul>
<li>the sum number of actions of the last day, last 3 days or last 7 days</li>
<li>the average number of actions of the last day, last 3 days or last 7 days</li>
<li>the average probability of the last day, last 3 days or last 7 days etc.</li>
</ul>
<p>In general, these operations are called window function and uses .rolling() function. For each row, the function looks at a window (# of rows around it) and applies a certain function to it. Currently, our data is on a userid and itemid level. First, we need to aggregate it on the level, we want to apply the window function.</p>
<p>We are interested in how much positive interaction an item had on the previous day. Next, we want to groupby our data frame by itemid. Then we apply the rolling function for two days (2D). Note: To use the rolling function with days, the data frame index has to by a timestamp. We can see that every row contains the sum of the row value + the previous row value. For example, itemid=1000001 for data 2020-01-02 counts 15 observations and sums 12 positive interactions.</p>
<p>If we take a look on the calculations, we see that the .rolling() inclues the value from the current row, as well. This could be a kind of data leakage. Therefore, we shift the values by one row.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Differences" duration="5">
        <p>Another category of powerful features is to calculate the differences to previous datapoints based on a timestamp. For example, we can calculate if the price changed of a product and how much the price change was. Tree-based or deep learning based models have difficulties processing these relationships on their own. Providing the models with these features can significantly improve the performance.</p>
<p>We can combine techniques of TimeSeries data and chain them together. For example, we can calculate the # of purchases per item and then compare the previous week with a the week, 2, 3 or 5 weeks ago. We can recognize patterns over time.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion" duration="2">
        <p>Congratulations!</p>
<h2 class="checklist" is-upgraded>What we&#39;ve covered</h2>
<ul class="checklist">
<li>ETL the raw data</li>
<li>Exploratory analysis</li>
<li>Feature engineering techniques</li>
<li>Time series feature techniques</li>
<li>Install and use RapidsAI&#39;s cuDF for faster processing</li>
</ul>
<h2 is-upgraded>Links and References</h2>
<ol type="1">
<li><a href="https://github.com/rapidsai/deeplearning/tree/main/RecSys2020Tutorial" target="_blank">https://github.com/rapidsai/deeplearning/tree/main/RecSys2020Tutorial</a></li>
<li><a href="https://nbviewer.jupyter.org/github/rapidsai/deeplearning/tree/main/RecSys2020Tutorial/" target="_blank">Official notebooks</a></li>
<li><a href="https://youtu.be/uROvhp7cj6Q?list=PLaZufLfJumb-cVIEsyg4CFocuq4WsvjED" target="_blank">Official video tutorial</a></li>
</ol>
<h2 is-upgraded>Have a Question?</h2>
<ul>
<li><a href="https://form.jotform.com/211377288388469" target="_blank">Fill out this form</a></li>
<li><a href="https://github.com/recohut/reco-step/issues" target="_blank">Raise issue on Github</a></li>
</ul>
<p>Beginner to intermediate users are the target audience, which should have prior knowledge in python programming using libraries, such as pandas and NumPy. In addition, they should have a basic understanding of recommender systems, decision trees and feed forward neural networks.</p>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
