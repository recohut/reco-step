
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>StoryCafe Series Part 1</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="storycafe-series-part-1"
                  title="StoryCafe Series Part 1"
                  environment="web"
                  feedback-link="https://github.com/recohut/reco-step/issues">
    
      <google-codelab-step label="Introduction" duration="5">
        <p>We are starting a new story series named <strong><em>StoryCafe</em></strong>. In this series, we are going to discuss stories that are not directly related to recommender systems but using machine learning and deep learning techniques to build and serve useful applications in the real world.</p>
<h2 is-upgraded>What you&#39;ll learn?</h2>
<ul>
<li>Build and deploy a binary classification model in clinical decision making</li>
<li>Build a chatbot and voicebot</li>
<li>Claim process automation</li>
<li>Multi-touch attribution for intelligent marketing budget allocation</li>
<li>Contact center analytics</li>
<li>Cyber risk quantification</li>
<li>ServiceNow advanced analytics</li>
<li>IT support ticket management</li>
<li>Claim severity modeling for re-insurance</li>
</ul>
<h2 is-upgraded>Why is this important?</h2>
<ul>
<li>This will help us in understanding the process of using ML/DL methods for building recommender systems</li>
</ul>
<h2 is-upgraded>How it will work?</h2>
<ul>
<li>We will briefly discuss the stories</li>
<li>With time, we will keep adding more details</li>
<li>We will also add source code and data/mock-up data wherever possible</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Clinical Decision Making" duration="5">
        <p>Health insurance can be complicated—especially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America&#39;s Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.</p>
<p>The objective was to identify the patterns from data to create clinical decision making in Pre-Auth and improve the accuracy in a clinical decision based on historical data analysis.</p>
<p>Two use cases were identified. Use Case 1 - <em>Supervised Learning Model - to aid clinicians in UM decision making. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Exploratory Data Analysis and Feature Engineering, Train supervised analytical models, model validation and model selection, Create a web service to be plugged into the case processing flow to call the model, and Display the recommendation from the model on UI on the authorization review screen. Use Case 2 - <em>Unsupervised Learning Model - to generate insights from the pre-authorization data. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Cluster analysis, univariate and multivariate analysis, and Generate insights and display insights on the dashboard.</p>
<p>Final Deliverables - Model re-training (batch mode), validation and deployment code (python scripts) with Unix command line support, Documentation - PPT, Recorded video, Technical document, Flask API backend system, HTML/PHP Web App frontend UI integration, and Plotly Dash Supervised/Unsupervised learning and insights generation dashboard.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Insurance Chatbot &amp; Voicebot" duration="5">
        <p>Problem statement: How to automate the insurance contact center services in order to reduce operational costs?, How to reduce the operational cost of contact center agents?, How to manage the SLAs of customer service in case of labor shortage?, How to reduce the resolution time of insurance customer&#39;s queries and complaints?</p>
<p>Scope: Insurance sector, automate 4 services - claim status check, claim filing, policy quotation, and policy education. The language would be English (US &amp; UK). RASA framework (python) was selected for the implementation of this chatbot.</p>
<p>A voice service was also ingegrated on the claim status check service. Various options were explored and simple proof-of-concepts were developed for each of them. A comparison chart was also created as shown below:</p>
<p class="image-container"><img alt="img/_markdowns-raw-recostep-stage-storycafe-series-part-1-untitled.png" src="img/514410061f597791.png"></p>
<p>Voicebot platform comparison</p>


      </google-codelab-step>
    
      <google-codelab-step label="Claim Process Automation" duration="5">
        <p>Today, in the car insurance industry, a lot of money is wasted due to claims leakage. Claims leakage / Underwriting leakage is defined as the difference between the actual claim payment made and the amount that should have been paid if all industry-leading practices were applied. Visual inspection and validation have been used to reduce such effects. However, they introduce delays in claim processing and lead to manual intervention. Fast and efficient claims processing is paramount to success for insurance companies.</p>
<p>Automatic assessment of the damages through image analysis is much faster and more accurate and it will become even better as they collect more &amp; more data for each use case. With deep learning, we can automatically detect scratches, dents, rust, breakages. We can also detect which part of the vehicle is damaged and with what severity. The vehicle can be automatically inspected using images or video feeds by creating a 360° overview. After the inspection, the report can be generated with a list of damages and estimate cost repair.</p>
<p>Identified solutions: Customer uploads an image of their damaged vehicle on the web portal/mobile app. Image Forgery Verification - Because there is a possibility of fraud by uploading fake/forged images, this model will verify if image is real or forged by some software. Vehicle Identity Verification - This model will verify if the damaged vehicle in image is the same vehicle for which the claim has been filed (damaged image of similar looking vehicle might be used to fraud the system). Data Sufficiency Verification - We will verify if images are of correct quality (not blurred, taken from the right angle) and also sufficient (enough number of images as reuqired by the model for correctly assess the damage). Distance Adjustment - We will adjust the distance of perspective to normalize the damage area pixels in the image. Depth Analysis - We will also measure the depth of the damage. Image Segmentation - This model will classify the pixels of the image into damage categories. We will use this information to assess the damage.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Multi-touch Attribution" duration="5">
        <p>We are living in the digital economy and a customer often exposed to promotional ads on different digital channels like Facebook and google search. If a customer purchased a product after multiple exposures on different channels, then how can we estimate each channel&#39;s individual contribution towards that conversion, so that we can assign the marketing budget accordingly?  <strong>How to quantify the impact of different media channels on sales volume?</strong></p>
<p>Multi-touch attribution is a set of methods and modeling techniques that tries to estimate the digital channel&#39;s individual contribution leveraging historical data of customer touchpoints and machine learning methodologies. I started with searching for relevant research papers in this domain, performed a literature review, and created a framework. I experimented with a lot of models and finally settled with three: Markov chains, Survival analysis, and RNN with attention. Conversion prediction accuracy was used as a proxy for evaluation and model selection but the A/B test should also be done before deploying this model in production.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Contact Center Analytics" duration="5">
        <p>Objectives: Increase Customer Satisfaction, Reduce Query Resolution Time, Reduce Operational Cost, and Increase Gross Profit.</p>
<p>Identified solutions - Increase customer satisfaction by 1) reduce the waiting time, and 2) Connect with the right agent. This can be achieved by intelligent customer routing analytics. Reduce query resolution time by 1) Automated query resolution, 2) Connect with the right agent, and 3) Agent real-time assistance. This can be achieved by the chatbot and intelligent routing analytics. Reduce operational cost by 1) hiring the optimal number of agents, and 2) customer segmentation and prioritization. This can be achieved by contact volume forecasting and agent staffing analytics. Increase gross profit by 1) Identifying churning customers and retention, and 2) Cross-sell and upsell during inbound contact. This can be achieved by churn modeling and cross-sell/up-sell modeling.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Cyber Risk Quantification" duration="5">
        <p>Economic and commercial operations have become increasingly reliant on digital technologies which face a constant threat of disruption due to human error or malicious attacks. The potential for serious economic and commercial repercussions, illustrated most recently in the millions of compromised records at Yahoo and Equifax, the disruption of major websites by a denial-of-service attack on Dyn and the hundreds of thousands of computers compromised by the WannaCry and NotPetya ransomware attacks, has meant increasing investment in safeguarding the confidentiality, integrity and availability of information and information systems.</p>
<p>The usual approach of managing information security risk is similar to other business risks, i.e. first eliminate, then mitigate, absorb, and then if possible, transfer. Since eliminating security risks in today&#39;s environment is not possible, managers deploy protection technologies like firewall, antivirus, encryption, and instate appropriate security policies like passwords, access control, port blocking, etc. to mitigate the probability of a break-in or failure. If the residual risk is manageable it is absorbed, otherwise, transferred by either outsourcing security or buying insurance.</p>
<p>While not a substitute for investing in cybersecurity and risk management—as having good cybersecurity and avoiding disruption is a more preferable outcome— insurance coverage for cyber risk can make an important contribution to the management of cyber risk by promoting awareness about exposure to cyber losses, sharing expertise on risk management, encouraging investment in risk reduction and facilitating the response to cyber incidents. <strong>How to quantify the cyber risk of a company for profitable ratemaking?</strong></p>
<p>Identified solutions: Baseline cyber risk quantification using FAIR framework including Beta PERT and Monte Carlo calibration, Data-driven IT security posture assessment using LSTM and CNN deep learning models, Macro environment risk adjustment by advanced frequency, severity and copula dependency modeling on historical data, and Accumulation risk adjustment using scenario-based portfolio risk assessment methodologies.</p>


      </google-codelab-step>
    
      <google-codelab-step label="ServiceNow Advanced Analytics" duration="5">
        <p>An insurance provider started using ServiceNow a few years ago, to catalog and process customer service requests. The incident descriptions are being analyzed manually to derive primary issues raised by customers, communicated with stakeholders. As the client grew rapidly, it was flooded with huge volumes of customer service requests. Eventually, the requests branched out into multiple service lines. The client has a wealth of text-based data sources, especially from ServiceNow, that require manual review in order to <strong>determine themes and emerging trends</strong>. The client wanted to <strong>remove the dependency on human-based reviews</strong> and extract more value from the wealth of information.</p>
<p>Initial set of tasks: Explore data science approaches for extracting value from our text based information, Grouping together similar phrases (e.g. incident summary, description, resolution notes), thereby grouping incident records of similar nature – using the same grouping mechanism, this could lead to the best matching set of existing problem records, Excluding common words/terms – a lot of tickets are raised via templates and have very similar wording – this provides very little value (consider Term Frequency / Inverse Document Frequency approach that reduces the search value of commonly used words), Consider stemming algorithms and others to align similar words (e.g. broken, not working, unavailable, not available), and Analytical and visualisation techniques to improve detection of emerging trends.</p>
<p>Identified solutions: Loading data from MS SQL Server onto to python, Restructuring data to enrich it and make it available in more usable form - Consolidating various data tables into a single data layer, Heatmap of month-wise incident volume for top service lines,  Custom stopwords removal, Lemmatization, Lower-casing, Tokenization, RAKE to capture emerging themes from textual data - Topic modeling to understand relationship between themes captured, Design various dashboard screens to demonstrate outlier analysis, word cloud comparison, the relationship between emerging themes, and Create a task scheduler to execute on a weekly, monthly, annual basis.</p>


      </google-codelab-step>
    
      <google-codelab-step label="IT Support Ticket Management" duration="5">
        <p>In Helpdesk, almost 30–40% of incident tickets are not routed to the right team and the tickets keep roaming around and around and by the time it reaches the right team, the issue might have widespread and reached the top management inviting a lot of trouble. Let&#39;s say that users are having some trouble with printers. User calls help desk, he creates a ticket with IT Support, and they realize that they need to update a configuration in the user&#39;s system and then they resolve the ticket. What if 10 other users report the same issue. <strong>How to better manage these common issues using analytics tools and techniques?</strong></p>
<p>Identified solutions: Key Phrase Analysis - Identify key-phrases using RAKE algorithm. Analyze frequency and plot word cloud visual for most common phrases. Topic Modeling - Divide the phrases into topics using LDA/LSA algorithm. Ticket Classification - Classify ticket into categories using ML classification algorithm. Trend, Seasonality, and Outlier Analysis - Weekly/ Monthly/ Quarterly/ Yearly trend in ticket volume using Moving Average and other similar algorithms. Identify outliers using IQR and other similar methods. PowerBI Dashboard - To visually represent the KPIs. Standard vs. Non-standard Template Responses - If the issue is getting repeated in multiple tickets (above a threshold), then it is labeled as standard-template.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Claim Severity for Reinsurance" duration="5">
        <p>In a typical insurance claims scenario, the insurance company gets a large number of small amounts of claims and few high amounts of claims. In statistical terms, we call it a body-trail curve, where the area under the curve represents the probability of a particular (x-axis) claim amount. So tail signifies a low probability but very high amount (severity) claims. Since there are only a few historical events in the history of this level of severity, how can we properly fit a model on this kind of data in order to correctly estimate the probability (area under the fitted curve) of the severity-level?</p>
<p>I experimented with different kinds of data transformation scales (log-scale, double-log scale, log-sigma scale, etc.) and multiple models (negative exponential, gamma, beta, Gumbel, etc.) and finally settled with a spliced curve model: multiple erlangs on body and Gumbel (from EVT class) on the tail. The AIC and BIC criteria were used for evaluation and model evaluation.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion" duration="2">
        <p>Congratulations!</p>
<h2 is-upgraded>Have a Question?</h2>
<ul>
<li><a href="https://form.jotform.com/211377288388469" target="_blank">Fill out this form</a></li>
<li><a href="https://github.com/recohut/reco-step/issues" target="_blank">Raise issue on Github</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
